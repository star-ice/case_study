{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"textcnn.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NRz39qvb2CZY","executionInfo":{"status":"ok","timestamp":1629100403315,"user_tz":-480,"elapsed":21455,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"51ec8f08-3e97-4c2b-e7f8-ac58be33a923"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"code_folding":[3],"id":"ZRnl443i1ttR","executionInfo":{"status":"ok","timestamp":1629100424547,"user_tz":-480,"elapsed":5070,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["# -*- coding: utf-8 -*-\n","'''\n","Use TextCNN to learn the feature of COURT_HELD text\n","\n","# 改为关键词匹配\n","reason topics we have:\n","\t标签、配料表、外包装违规（包括虚假、夸大信息，格式、名称规范问题等等）;\n","\t假冒产品（假酒、假保健品之类的）;\n","\t保质期、生产日期;\n","\t原材料、添加剂;\n","\t商标;\n","\t出入境检验、检疫证明;\n","\t生产许可证、生产标准、证明文件;\n","\t餐饮食品安全卫生标准;（只限餐饮）\n","\t是否为消费者（有些人是专业打假人）;\n","\t进口食品相关产品尚无国家标准（对尚无国家标准的食品或相关产品未做安全性评估就销售）;\n","\t产品质检不合格、质量有问题;\n","\t不明确（可以当做干扰项去掉）\n","标签优先提取\n","'''\n","import os\n","import numpy as np\n","import pandas as pd\n","import torchtext\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from collections import Counter\n","from torchtext.legacy import data\n","from sklearn.metrics import classification_report"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"EbWVtUox1ttW","executionInfo":{"status":"ok","timestamp":1629100431654,"user_tz":-480,"elapsed":7111,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"fe39e6d5-d9f8-4b61-fb92-3c42527f974d"},"source":["path1 = \"/content/drive/MyDrive/Colab Notebooks/textcnn/dataset/textcnn_dataset.csv\"\n","path2 = \"/content/drive/MyDrive/Colab Notebooks/textcnn/dataset/type1_2014_1-2-3-4-5-6-7-8-9-10-11-12_nps.csv\"\n","\n","all_classes = pd.read_csv(path1, encoding=\"utf-8\")\n","splitted_text = pd.read_csv(path2, encoding=\"utf-8\")\n","all_classes = all_classes[~all_classes['reason_topic'].isin([\"不明确\", \"unknown\"])][['id', 'reason_topic']]\n","splitted_text = splitted_text[['id', 'content', 'topic_phrases', 'phrase_vectors']]\n","all_cases = all_classes.merge(splitted_text, on=\"id\")\n","all_cases['reason_topic'] = all_cases['reason_topic'].apply(lambda row: row.split(\";\")[0])\n","all_cases.rename(columns={\"reason_topic\": \"label\"}, inplace=True)\n","\n","labels = list(all_cases['label'].unique())\n","print(\"All labels in this dataset are: \", labels)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["All labels in this dataset are:  ['原材料、添加剂', '产品质检不合格、质量有问题', '标签、配料表、外包装违规', '进口食品相关产品尚无国家标准', '保质期、生产日期', '生产许可证、生产标准、证明文件', '假冒产品', '餐饮食品安全卫生标准', '出入境检验、检疫证明', '商标', '是否为消费者']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"ic7OVELj1ttX","executionInfo":{"status":"ok","timestamp":1629100431655,"user_tz":-480,"elapsed":25,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"46bd2025-6398-4b21-d519-e0dd48cca2fb"},"source":["Counter(list(all_cases.label))\n","#存在有些标签数据量过少的现象"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'产品质检不合格、质量有问题': 21,\n","         '保质期、生产日期': 64,\n","         '假冒产品': 13,\n","         '出入境检验、检疫证明': 5,\n","         '原材料、添加剂': 50,\n","         '商标': 1,\n","         '是否为消费者': 1,\n","         '标签、配料表、外包装违规': 139,\n","         '生产许可证、生产标准、证明文件': 41,\n","         '进口食品相关产品尚无国家标准': 3,\n","         '餐饮食品安全卫生标准': 52})"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":666},"id":"9g75XbEg1ttY","executionInfo":{"status":"ok","timestamp":1629100431657,"user_tz":-480,"elapsed":24,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"207dd131-30fd-4554-fb0b-8506c31351a1"},"source":["# 先去掉商标，是否为消费者，进口食品相关产品尚无国家标准这三个类别，因为数据量太小了\n","all_cases = all_cases[~all_cases['label'].isin([\"商标\", \"是否为消费者\", \"进口食品相关产品尚无国家标准\"])]\n","print(Counter(list(all_cases.label)))\n","all_cases.head()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Counter({'标签、配料表、外包装违规': 139, '保质期、生产日期': 64, '餐饮食品安全卫生标准': 52, '原材料、添加剂': 50, '生产许可证、生产标准、证明文件': 41, '产品质检不合格、质量有问题': 21, '假冒产品': 13, '出入境检验、检疫证明': 5})\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>content</th>\n","      <th>topic_phrases</th>\n","      <th>phrase_vectors</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>57ab9058c2265c28a560195d</td>\n","      <td>原材料、添加剂</td>\n","      <td>上述认定 证据 当事人 陈述 本院 认定 事实 2012年8月16日 喻忠 淘宝网 天猫商城...</td>\n","      <td>['上述认定', '证据', '当事人', '本院', '事实', '淘宝网天猫商城', '...</td>\n","      <td>[b'~\\x8c\\x1f@\\x88\\xf4\\x17\\xc0\\x98\\xf5B\\xbf;S0?...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>57baba28c2265c5f452d2cef</td>\n","      <td>产品质检不合格、质量有问题</td>\n","      <td>本院认为,原告购买的是被告 散装茶叶 选用 被告 店 市场 流行 通用包装盒 装 茶 原告 ...</td>\n","      <td>['散装茶叶', '市场', '通用包装盒', '收据', '当庭陈述', '质量安全管理'...</td>\n","      <td>[b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>57abb74ac2265c258984430a</td>\n","      <td>标签、配料表、外包装违规</td>\n","      <td>本院认为,食品安全标准 应 包括 食品安全 营养 标签 标识 说明书 食品经营者 查验 商品...</td>\n","      <td>['食品安全', '营养', '标签', '标识', '食品经营者', '商品合格', '证...</td>\n","      <td>[b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>57baba99c2265c5f452d3090</td>\n","      <td>产品质检不合格、质量有问题</td>\n","      <td>本院认为,原告购买的是被告 散装茶叶 选用 被告 店 市场 流行 通用包装盒 装 茶 原告 ...</td>\n","      <td>['散装茶叶', '市场', '通用包装盒', '收据', '当庭陈述', '质量安全管理'...</td>\n","      <td>[b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>57baba8fc2265c5f452d303f</td>\n","      <td>产品质检不合格、质量有问题</td>\n","      <td>本院认为,原告购买的是被告 散装茶叶 选用 被告 店 市场 流行 通用包装盒 装 茶 原告 ...</td>\n","      <td>['散装茶叶', '市场', '通用包装盒', '收据', '当庭陈述', '质量安全管理'...</td>\n","      <td>[b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         id  ...                                     phrase_vectors\n","0  57ab9058c2265c28a560195d  ...  [b'~\\x8c\\x1f@\\x88\\xf4\\x17\\xc0\\x98\\xf5B\\xbf;S0?...\n","1  57baba28c2265c5f452d2cef  ...  [b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...\n","2  57abb74ac2265c258984430a  ...  [b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0...\n","4  57baba99c2265c5f452d3090  ...  [b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...\n","6  57baba8fc2265c5f452d303f  ...  [b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"2gW2DuNU1ttY","executionInfo":{"status":"ok","timestamp":1629100500912,"user_tz":-480,"elapsed":1230,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"3b2cf864-453b-4ae6-d6cc-bcd0fe0dcbf6"},"source":["# 每个类别都按照311划分为train validation和test，将这三类文件输出\n","textcnn_bpath = \"/content/drive/MyDrive/Colab Notebooks/textcnn/dataset\"\n","train = pd.DataFrame(columns=[\"label\", \"content\"])\n","val = pd.DataFrame(columns=[\"label\", \"content\"])\n","test = pd.DataFrame(columns=[\"label\", \"content\"])\n","for i in list(all_cases.label.unique()):\n","    print(i)\n","    train_len = int(len(all_cases[all_cases['label']==i]) * 0.6)\n","    val_len = int(len(all_cases[all_cases['label']==i]) * 0.2)\n","    # test_len = len(all_cases[all_cases['label']==i]) * 0.2\n","    print(train_len, val_len)\n","    train = train.append(all_cases[all_cases['label']==i][:train_len][['label', 'content']], ignore_index=True)\n","    val = val.append(all_cases[all_cases['label']==i][train_len:train_len+val_len][['label', 'content']], ignore_index=True)\n","    test = test.append(all_cases[all_cases['label']==i][train_len+val_len:][['label', 'content']], ignore_index=True)\n","with open(textcnn_bpath+\"train.tsv\",'w') as write_tsv:\n","    write_tsv.write(train.to_csv(sep='\\t', index=False))\n","with open(textcnn_bpath+\"val.tsv\",'w') as write_tsv:\n","    write_tsv.write(val.to_csv(sep='\\t', index=False))\n","with open(textcnn_bpath+\"test.tsv\",'w') as write_tsv:\n","    write_tsv.write(test.to_csv(sep='\\t', index=False))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["原材料、添加剂\n","30 10\n","产品质检不合格、质量有问题\n","12 4\n","标签、配料表、外包装违规\n","83 27\n","保质期、生产日期\n","38 12\n","生产许可证、生产标准、证明文件\n","24 8\n","假冒产品\n","7 2\n","餐饮食品安全卫生标准\n","31 10\n","出入境检验、检疫证明\n","3 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"UHVq-tiJ1ttZ","executionInfo":{"status":"ok","timestamp":1629101214994,"user_tz":-480,"elapsed":804,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["#声明一个Field对象，对象里面填的就是需要对文本进行哪些操作，比如这里lower=True英文大写转小写,tokenize=cut对于文本分词采用之前定义好的cut函数，sequence=True表示输入的是一个sequence类型的数据，还有其他更多操作可以参考文档\n","TEXT = data.Field(sequential=True,lower=True)\n","#声明一个标签的LabelField对象，sequential=False表示标签不是sequence，dtype=torch.int64标签转化成整形\n","LABEL = data.LabelField(sequential=False, dtype=torch.int64)\n","\n","#这里主要是告诉torchtext需要处理哪些数据，这些数据存放在哪里，TabularDataset是一个处理scv/tsv的常用类\n","train_dataset, dev_dataset, test_dataset = data.TabularDataset.splits(\n","      path=textcnn_bpath,  #文件存放路径\n","      format='tsv',   #文件格式\n","      skip_header=True,  #是否跳过表头，我这里数据集中没有表头，所以不跳过\n","      train='train.tsv',\n","      validation='val.tsv',\n","      test='test.tsv',\n","      fields=[('label',LABEL),('content',TEXT)] # 定义数据对应的表头\n",")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"Mf1gfggn1tta","executionInfo":{"status":"ok","timestamp":1629101223521,"user_tz":-480,"elapsed":7249,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["# 预训练词向量\n","pretrained_name = 'sgns.baidubaike.bigram-char' # 预训练词向量文件名\n","pretrained_path = textcnn_bpath #预训练词向量存放路径\n","vectors = torchtext.vocab.Vectors(name=pretrained_name, cache=pretrained_path)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"3CEK6jyz1ttb","executionInfo":{"status":"ok","timestamp":1629101223523,"user_tz":-480,"elapsed":13,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["# 建立词表\n","TEXT.build_vocab(train_dataset,\n","                 dev_dataset,\n","                 test_dataset,\n","                 vectors=vectors)\n","LABEL.build_vocab(train_dataset,\n","                  dev_dataset,\n","                  test_dataset)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"f_WnCv6M1ttc","executionInfo":{"status":"ok","timestamp":1629095746401,"user_tz":-480,"elapsed":404,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"a1a39475-0351-48ee-a3fa-c30077350e57"},"source":["# 查看词表信息\n","print(\"词表中的词汇总量为： \", len(TEXT.vocab))\n","print(\"词向量维度为： \", TEXT.vocab.vectors.shape)\n","print(TEXT.vocab.stoi['过期食品'])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["词表中的词汇总量为：  8621\n","词向量维度为：  torch.Size([8621, 300])\n","544\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"HWg8JaWM1ttc","executionInfo":{"status":"ok","timestamp":1629101237277,"user_tz":-480,"elapsed":289,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["#生成迭代器\n","train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n","        (train_dataset, dev_dataset, test_dataset), #需要生成迭代器的数据集\n","        batch_sizes=(128, 128, 128), # 每个迭代器分别以多少样本为一个batch\n","        sort_key=lambda x: len(x.content) #按什么顺序来排列batch，这里是以句子的长度，就是上面说的把句子长度相近的放在同一个batch里面\n","        )"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"glyCW-9e1ttd","executionInfo":{"status":"ok","timestamp":1629101254972,"user_tz":-480,"elapsed":281,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["# TextCNN建模\n","class TextCNN(nn.Module):\n","    def __init__(self,\n","                 class_num, # 最后输出的种类数\n","                 filter_sizes, # 卷积核的长也就是滑动窗口的长\n","                 filter_num,   # 卷积核的数量\n","                 vocabulary_size, # 词表的大小\n","                 embedding_dimension, # 词向量的维度\n","                 vectors, # 词向量\n","                 dropout): # dropout率\n","        super(TextCNN, self).__init__() # 继承nn.Module\n","\n","        chanel_num = 1  # 通道数，也就是一篇文章一个样本只相当于一个feature map\n","\n","        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension) # 嵌入层\n","        self.embedding = self.embedding.from_pretrained(vectors) #嵌入层加载预训练词向量\n","\n","        self.convs = nn.ModuleList(\n","            [nn.Conv2d(chanel_num, filter_num, (fsz, embedding_dimension)) for fsz in filter_sizes])  # 卷积层\n","        self.dropout = nn.Dropout(dropout) # dropout\n","        self.fc = nn.Linear(len(filter_sizes) * filter_num, class_num) #全连接层\n","\n","    def forward(self, x):\n","        # x维度[句子长度,一个batch中所包含的样本数] 例:[3451,128]\n","        x = self.embedding(x) # #经过嵌入层之后x的维度，[句子长度,一个batch中所包含的样本数,词向量维度] 例：[3451,128,300]\n","        x = x.permute(1,0,2) # permute函数将样本数和句子长度换一下位置，[一个batch中所包含的样本数,句子长度,词向量维度] 例：[128,3451,300]\n","        x = x.unsqueeze(1) # # conv2d需要输入的是一个四维数据，所以新增一维feature map数 unsqueeze(1)表示在第一维处新增一维，[一个batch中所包含的样本数,一个样本中的feature map数，句子长度,词向量维度] 例：[128,1,3451,300]\n","        x = [conv(x) for conv in self.convs] # 与卷积核进行卷积，输出是[一个batch中所包含的样本数,卷积核数，句子长度-卷积核size+1,1]维数据,因为有[3,4,5]三张size类型的卷积核所以用列表表达式 例：[[128,16,3459,1],[128,16,3458,1],[128,16,3457,1]]\n","        x = [sub_x.squeeze(3) for sub_x in x]#squeeze(3)判断第三维是否是1，如果是则压缩，如不是则保持原样 例：[[128,16,3459],[128,16,3458],[128,16,3457]]\n","        x = [F.relu(sub_x) for sub_x in x] # ReLU激活函数激活，不改变x维度\n","        x = [F.max_pool1d(sub_x,sub_x.size(2)) for sub_x in x] # 池化层，根据之前说的原理，max_pool1d要取出每一个滑动窗口生成的矩阵的最大值，因此在第二维上取最大值 例：[[128,16,1],[128,16,1],[128,16,1]]\n","        x = [sub_x.squeeze(2) for sub_x in x] # 判断第二维是否为1，若是则压缩 例：[[128,16],[128,16],[128,16]]\n","        x = torch.cat(x, 1) # 进行拼接，例：[128,48]\n","        x = self.dropout(x) # 去除掉一些神经元防止过拟合，注意dropout之后x的维度依旧是[128,48]，并不是说我dropout的概率是0.5，去除了一半的神经元维度就变成了[128,24]，而是把x中的一些神经元的数据根据概率全部变成了0，维度依旧是[128,48]\n","        logits = self.fc(x) # 全接连层 例：输入x是[128,48] 输出logits是[128,10]\n","        return logits"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"_KUHvhwf1ttd","executionInfo":{"status":"ok","timestamp":1629101257482,"user_tz":-480,"elapsed":288,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["# 模型训练\n","class_num = len(LABEL.vocab) # 类别数目\n","filter_size = [3,4,5]  # 卷积核种类数\n","filter_num=16   # 卷积核数量\n","vocab_size = len(TEXT.vocab) # 词表大小\n","embedding_dim = TEXT.vocab.vectors.size()[-1] # 词向量维度\n","vectors = TEXT.vocab.vectors # 词向量\n","dropout=0.5\n","learning_rate = 0.001  # 学习率\n","epochs = 300   # 迭代次数\n","save_dir = '/content/drive/MyDrive/Colab Notebooks/textcnn/model' # 模型保存路径\n","steps_show = 10   # 每10步查看一次训练集loss和mini batch里的准确率\n","steps_eval = 100  # 每100步测试一下验证集的准确率\n","early_stopping = 1000  # 若发现当前验证集的准确率在1000步训练之后不再提高 一直小于best_acc,则提前停止训练\n","\n","textcnn_model = TextCNN(class_num=class_num,\n","        filter_sizes=filter_size,\n","        filter_num=filter_num,\n","        vocabulary_size=vocab_size,\n","        embedding_dimension=embedding_dim,\n","        vectors=vectors,\n","        dropout=dropout)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"YeRZ8Otm1tte","executionInfo":{"status":"ok","timestamp":1629101223523,"user_tz":-480,"elapsed":12,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["# 模型验证方法\n","def dev_eval(dev_iter, model):\n","    model.eval()\n","    corrects, avg_loss = 0, 0\n","    for batch in dev_iter:\n","        feature, target = batch.content, batch.label\n","        if torch.cuda.is_available():\n","            feature, target = feature.cuda(), target.cuda()\n","        logits = model(feature)\n","        # print(logits, target)\n","        loss = F.cross_entropy(logits, target)\n","        avg_loss += loss.item()\n","        pred_tags = torch.max(logits, 1)\n","        corrects += (torch.max(logits, 1)\n","                    [1].view(target.size()).data == target.data).sum()\n","    size = len(dev_iter.dataset)\n","    avg_loss /= size\n","    accuracy = 100.0 * corrects / size\n","    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss,\n","                                                                          accuracy,\n","                                                                          corrects,\n","                                                                          size))\n","    return accuracy, target, pred_tags"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"kjedjtBj1ttf","executionInfo":{"status":"ok","timestamp":1629101289619,"user_tz":-480,"elapsed":288,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["# 定义模型保存函数\n","def save(model, save_dir, steps):\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","    save_path = 'bestmodel_steps{}.pt'.format(steps)\n","    save_bestmodel_path = os.path.join(save_dir, save_path)\n","    torch.save(model.state_dict(), save_bestmodel_path)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"zWUjntWQ1ttf","executionInfo":{"status":"ok","timestamp":1629101291706,"user_tz":-480,"elapsed":414,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["def train(train_iter, dev_iter, model):\n","\n","    if torch.cuda.is_available(): # 判断是否有GPU，如果有把模型放在GPU上训练，速度质的飞跃\n","        model.cuda()\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # 梯度下降优化器，采用Adam\n","    steps = 0\n","    best_acc = 0\n","    last_step = 0\n","    model.train()\n","    for epoch in range(1, epochs + 1):\n","        for batch in train_iter:\n","            feature, target = batch.content, batch.label\n","            if torch.cuda.is_available(): # 如果有GPU将特征更新放在GPU上\n","                feature,target = feature.cuda(),target.cuda()\n","            optimizer.zero_grad() # 将梯度初始化为0，每个batch都是独立训练地，因为每训练一个batch都需要将梯度归零\n","            logits = model(feature)\n","            loss = F.cross_entropy(logits, target) # 计算损失函数 采用交叉熵损失函数\n","            loss.backward()  # 反向传播\n","            optimizer.step() # 放在loss.backward()后进行参数的更新\n","            steps += 1\n","            if steps % steps_show == 0: # 每训练多少步计算一次准确率，我这边是100，可以自己修改\n","                corrects = (torch.max(logits, 1)[1].view(target.size()).data == target.data).sum() # logits是[128,10],torch.max(logits, 1)也就是选出第一维中概率最大的值，输出为[128,1],torch.max(logits, 1)[1]相当于把每一个样本的预测输出取出来，然后通过view(target.size())平铺成和target一样的size (128,),然后把与target中相同的求和，统计预测正确的数量\n","                train_acc = 100.0 * corrects / batch.batch_size # 计算每个mini batch中的准确率\n","                print('steps:{} - loss: {:.6f}  acc:{:.4f}'.format(\n","                  steps,\n","                  loss.item(),\n","                  train_acc))\n","\n","            if steps % steps_eval == 0: # 每训练100步进行一次验证\n","                dev_acc, target, pred_tags = dev_eval(dev_iter,model)\n","                print(\"dev_acc: \", dev_acc)\n","                if dev_acc > best_acc :\n","                    best_acc = dev_acc\n","                    last_step = steps\n","                    print('Saving best model, acc: {:.4f}%\\n'.format(best_acc))\n","                    save(model, save_dir, steps)\n","                else:\n","                    if steps - last_step >= early_stopping:\n","                        print('\\n提前停止于 {} steps, acc: {:.4f}%'.format(last_step, best_acc))\n","                        raise KeyboardInterrupt"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"is_executing":true,"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"BU7qrP-w1ttg","executionInfo":{"status":"ok","timestamp":1629101317390,"user_tz":-480,"elapsed":23594,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"1af71552-4fbf-4d0c-bb0f-a5d4cd78966f"},"source":["#训练\n","train(train_iter, dev_iter, textcnn_model)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"],"name":"stderr"},{"output_type":"stream","text":["steps:10 - loss: 1.440770  acc:49.0000\n","steps:20 - loss: 1.157834  acc:67.0000\n","steps:30 - loss: 0.972232  acc:69.5312\n","steps:40 - loss: 0.827206  acc:77.3438\n","steps:50 - loss: 0.695821  acc:81.0000\n","steps:60 - loss: 0.542120  acc:88.0000\n","steps:70 - loss: 0.585213  acc:82.8125\n","steps:80 - loss: 0.617400  acc:83.0000\n","steps:90 - loss: 0.512736  acc:89.0000\n","steps:100 - loss: 0.410736  acc:92.0000\n","\n","Evaluation - loss: 0.012838  acc: 70.2703%(52/74) \n","\n","dev_acc:  tensor(70.2703, device='cuda:0')\n","Saving best model, acc: 70.2703%\n","\n","steps:110 - loss: 0.220794  acc:97.0000\n","steps:120 - loss: 0.155750  acc:98.4375\n","steps:130 - loss: 0.124715  acc:98.4375\n","steps:140 - loss: 0.100286  acc:100.0000\n","steps:150 - loss: 0.065298  acc:100.0000\n","steps:160 - loss: 0.095035  acc:98.0000\n","steps:170 - loss: 0.067119  acc:99.0000\n","steps:180 - loss: 0.086792  acc:97.0000\n","steps:190 - loss: 0.040254  acc:99.2188\n","steps:200 - loss: 0.042255  acc:99.0000\n","\n","Evaluation - loss: 0.012553  acc: 68.9189%(51/74) \n","\n","dev_acc:  tensor(68.9189, device='cuda:0')\n","steps:210 - loss: 0.036989  acc:98.4375\n","steps:220 - loss: 0.047711  acc:98.0000\n","steps:230 - loss: 0.034125  acc:98.4375\n","steps:240 - loss: 0.022485  acc:100.0000\n","steps:250 - loss: 0.033868  acc:99.2188\n","steps:260 - loss: 0.032451  acc:99.0000\n","steps:270 - loss: 0.028431  acc:99.0000\n","steps:280 - loss: 0.038494  acc:98.0000\n","steps:290 - loss: 0.038962  acc:98.0000\n","steps:300 - loss: 0.033116  acc:99.2188\n","\n","Evaluation - loss: 0.013139  acc: 66.2162%(49/74) \n","\n","dev_acc:  tensor(66.2162, device='cuda:0')\n","steps:310 - loss: 0.027170  acc:99.0000\n","steps:320 - loss: 0.040119  acc:98.0000\n","steps:330 - loss: 0.038475  acc:99.2188\n","steps:340 - loss: 0.041436  acc:98.4375\n","steps:350 - loss: 0.031078  acc:98.0000\n","steps:360 - loss: 0.046897  acc:98.0000\n","steps:370 - loss: 0.046887  acc:97.6562\n","steps:380 - loss: 0.018111  acc:100.0000\n","steps:390 - loss: 0.014626  acc:100.0000\n","steps:400 - loss: 0.055681  acc:97.0000\n","\n","Evaluation - loss: 0.013559  acc: 66.2162%(49/74) \n","\n","dev_acc:  tensor(66.2162, device='cuda:0')\n","steps:410 - loss: 0.022025  acc:99.0000\n","steps:420 - loss: 0.018849  acc:99.0000\n","steps:430 - loss: 0.048135  acc:97.6562\n","steps:440 - loss: 0.043741  acc:97.6562\n","steps:450 - loss: 0.026161  acc:98.4375\n","steps:460 - loss: 0.009053  acc:100.0000\n","steps:470 - loss: 0.022151  acc:98.4375\n","steps:480 - loss: 0.042725  acc:98.0000\n","steps:490 - loss: 0.047238  acc:97.6562\n","steps:500 - loss: 0.047448  acc:97.6562\n","\n","Evaluation - loss: 0.013960  acc: 66.2162%(49/74) \n","\n","dev_acc:  tensor(66.2162, device='cuda:0')\n","steps:510 - loss: 0.029233  acc:98.4375\n","steps:520 - loss: 0.018240  acc:99.0000\n","steps:530 - loss: 0.056317  acc:97.0000\n","steps:540 - loss: 0.028992  acc:98.0000\n","steps:550 - loss: 0.047349  acc:98.0000\n","steps:560 - loss: 0.008547  acc:100.0000\n","steps:570 - loss: 0.042090  acc:97.6562\n","steps:580 - loss: 0.007423  acc:100.0000\n","steps:590 - loss: 0.020240  acc:98.4375\n","steps:600 - loss: 0.049099  acc:97.0000\n","\n","Evaluation - loss: 0.014148  acc: 66.2162%(49/74) \n","\n","dev_acc:  tensor(66.2162, device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fe11ZUj61ttg","executionInfo":{"status":"ok","timestamp":1629101321279,"user_tz":-480,"elapsed":280,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"15a6cff3-46ad-49b5-d208-3c2bc6991e2c"},"source":["dev_acc, target, pred_tags = dev_eval(test_iter, textcnn_model)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["\n","Evaluation - loss: 0.013752  acc: 71.0843%(59/83) \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dUFWqEHGFJY","executionInfo":{"status":"ok","timestamp":1629101330421,"user_tz":-480,"elapsed":291,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"658d7874-092b-45e1-b39d-eb9ba28a0259"},"source":["print(classification_report(pred_tags[1].cpu(), target.cpu()))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.93      0.66      0.77        41\n","           1       0.93      0.68      0.79        19\n","           2       1.00      1.00      1.00        11\n","           3       0.50      0.71      0.59         7\n","           4       0.33      0.75      0.46         4\n","           5       0.00      0.00      0.00         1\n","           6       0.00      0.00      0.00         0\n","           7       0.00      0.00      0.00         0\n","\n","    accuracy                           0.71        83\n","   macro avg       0.46      0.48      0.45        83\n","weighted avg       0.86      0.71      0.77        83\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dDHq6y_9NWgE","executionInfo":{"status":"ok","timestamp":1629101486759,"user_tz":-480,"elapsed":300,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}},"outputId":"37f4e2d9-57ca-45e2-8c7e-688c36302042"},"source":["print(pred_tags[1].cpu())\n","for i in pred_tags[1].cpu()[:]:\n","  print(LABEL.vocab.itos[i])"],"execution_count":25,"outputs":[{"output_type":"stream","text":["tensor([0, 1, 3, 0, 3, 1, 0, 1, 3, 0, 0, 1, 0, 1, 0, 3, 1, 0, 5, 1, 0, 0, 0, 1,\n","        4, 1, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 3, 1, 0, 3, 1,\n","        0, 1, 0, 1, 1, 3, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2,\n","        2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1])\n","标签、配料表、外包装违规\n","保质期、生产日期\n","原材料、添加剂\n","标签、配料表、外包装违规\n","原材料、添加剂\n","保质期、生产日期\n","标签、配料表、外包装违规\n","保质期、生产日期\n","原材料、添加剂\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","保质期、生产日期\n","标签、配料表、外包装违规\n","保质期、生产日期\n","标签、配料表、外包装违规\n","原材料、添加剂\n","保质期、生产日期\n","标签、配料表、外包装违规\n","产品质检不合格、质量有问题\n","保质期、生产日期\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","保质期、生产日期\n","生产许可证、生产标准、证明文件\n","保质期、生产日期\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","生产许可证、生产标准、证明文件\n","生产许可证、生产标准、证明文件\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","生产许可证、生产标准、证明文件\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","保质期、生产日期\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","原材料、添加剂\n","保质期、生产日期\n","标签、配料表、外包装违规\n","原材料、添加剂\n","保质期、生产日期\n","标签、配料表、外包装违规\n","保质期、生产日期\n","标签、配料表、外包装违规\n","保质期、生产日期\n","保质期、生产日期\n","原材料、添加剂\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","保质期、生产日期\n","标签、配料表、外包装违规\n","保质期、生产日期\n","保质期、生产日期\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","标签、配料表、外包装违规\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","餐饮食品安全卫生标准\n","标签、配料表、外包装违规\n","保质期、生产日期\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iNsCzv9DMEOT","executionInfo":{"status":"ok","timestamp":1629088780935,"user_tz":-480,"elapsed":254,"user":{"displayName":"Starice Lian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi653O6zJ4mGo4WhsKb2WrG3p196N4siY8sL2bN=s64","userId":"01121734706915995871"}}},"source":["# TODO\n","# # 使用模型对现有数据做预测\n","# # 以2015年的第一类数据为例预测一下\n","# def pred(dev_iter, model):\n","#   for batch in dev_iter:\n","#         feature, target = batch.content, batch.label\n","#         if torch.cuda.is_available():\n","#             feature, target = feature.cuda(), target.cuda()\n","#         logits = model(feature)\n","#         # print(logits, target)\n","#         loss = F.cross_entropy(logits, target)\n","#         avg_loss += loss.item()\n","#         pred_tags = torch.max(logits, 1)\n","#         corrects += (torch.max(logits, 1)\n","#                     [1].view(target.size()).data == target.data).sum()"],"execution_count":75,"outputs":[]}]}