{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>This notebook is to extract the detailed reason keywords in each case file.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "import zh_core_web_trf\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "from spacy.attrs import ORTH\n",
    "import re\n",
    "from spacy.tokens import Doc\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy import displacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.symbols import POS\n",
    "from spacy.strings import StringStore\n",
    "from spacy.pipeline import Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"/Users/starice/OwnFiles/cityu/RA/\"\n",
    "pre_dir = ['type1', 'type2', 'type3', 'type4']\n",
    "dir_name = ['2014', '2015', '2016', '2017', '2018', '2019', '2020']\n",
    "dir_sname = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目前发现的问题\n",
    "# 1. 如果法院认为内容中的法律条款内容里出现了reason keywords，并不能说明该案件就和出现的keyword有关联，需要再做区分才行。\n",
    "#     例如type1 2014 1 json: 5911d51ec3666e1b15607ff5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 想到的解决方法\n",
    "# 1. 原文内容匹配，从文本中找出食品安全法相关的文本并和食品安全法原文做匹配，进一步探寻原因\n",
    "#     有些案件只标明违反了第九十六条，具体原因不明\n",
    "# 2. 老老实实用关键字匹配，不是所有的文本都标明了违反的具体条款\n",
    "#     用匹配到的关键字出现的次数来决定最终属于哪几类违法原因（在有多于两个案件原因的情况下，\n",
    "#     如果某些原因的关键字出现次数少于两次，就认为是混淆项，可以排除掉）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build Reason Dictionary</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "# reason1: 标签、配料表、外包装违规（包括虚假、夸大信息，格式、名称规范问题等等）\n",
    "# reason2: 假冒产品\n",
    "# reason3: 保质期、生产日期\n",
    "# reason4: 原材料、添加剂\n",
    "# reason5: 商标\n",
    "# reason6: 出入境检验、检疫证明、来源证明（进口食品或野生动植物）\n",
    "# reason7: 生产许可证、生产标准、证明文件\n",
    "# reason8: 餐饮食品安全卫生标准（只限餐饮）\n",
    "# reason9: 是否为真实消费者（疑似打假人）\n",
    "# reason10: 食品相关产品尚无国家标准或相关产品未通过安全性评估\n",
    "# reason11: 产品质检不合格、质量有问题\n",
    "# keyword1: 进口食品（不是单独的类别，只和普通食品的违法原因区分）\n",
    "\n",
    "reasons = {\n",
    "    \"reason1\": \n",
    "        ['标签', '产品标签营养成分表内容', '外包装食品标签', \n",
    "         '标记', '标识', '标题', '条形码', '食品外包装食品标签', \n",
    "         '标注', '卷标', '产品说明标签', '标签不符合', '标签不合格', \n",
    "         '未标注配料表', '食品配料表', '外包装配料表', '配料表未标注', \n",
    "         '外包装', '虚假', '虚假欺诈', '夸大', '夸大宣传', \n",
    "         '欺骗误导', '吹捧', '贬低', '夸大其词', '标签格式', '中文标签格式', \n",
    "         '中文标签版面格式', '营养标签格式', '文字格式包装', \n",
    "         '标签字体', '名称规范', '产品标准号', '说明书'], \n",
    "    \"reason2\": \n",
    "        ['假冒产品', '假冒伪劣产品', '假冒侵权产品', '伪劣产品'], \n",
    "    \"reason3\": \n",
    "        ['保质期', '保质期限', '生产日期', '过期'], \n",
    "    \"reason4\": \n",
    "        ['原始配料', '原料', '原材料', '原料未标注', \n",
    "         '非药食同源物质', '食品添加剂', '添加剂', '非食用', \n",
    "         '非食品', '食品原料'], \n",
    "    \"reason5\": \n",
    "        ['商标', '商标专用权', '商标违法', '商标侵权'], \n",
    "    \"reason6\": \n",
    "        ['出入境检验', '出入境检疫', '入境', '边检', \n",
    "         '入境检验', '入境检验检疫', '海关检疫', '检疫', '来源证明'], \n",
    "    \"reason7\": \n",
    "        ['生产许可', '许可生产', '生产经营许可'], \n",
    "    \"reason8\": \n",
    "        ['餐饮', '餐饮服务', '餐饮服务场所', '餐饮服务提供者', \n",
    "         '餐饮服务许可', '餐饮店', '餐饮食品安全卫生标准', \n",
    "         '食物中毒', '食源性疾病', '小吃', '饭店', '饭馆', \n",
    "         '火锅', '烧烤', '酒店', '人身安全', '财产安全'], \n",
    "    \"reason9\": \n",
    "        ['真实消费者', '是否为消费者', '是否是消费者'], \n",
    "    \"reason10\": \n",
    "        ['无国家标准', '安全性评估', '安全性评估审查', \n",
    "         '相关安全性评估', '安全性评估材料', '安全性评估报告', \n",
    "         '安全性评价', '安全评估', '质量评估'], \n",
    "    \"reason11\": \n",
    "        ['质检不合格', '质量不符合', '不符合质量', \n",
    "         '质量品质', '质量保证书', '质量合格证', '性状', \n",
    "         '外观性状', '感官性状', '化学性状', '产品性状', \n",
    "         '基本自然性状', '品系', '人身安全', '财产安全', \n",
    "         '检验', '检测', '证明材料', '供应商资质'], \n",
    "    \"keyword1\": \n",
    "        ['进口', '进出口', '进出口许可', \n",
    "         '进出口资质', '对外贸易', '食品进口', \n",
    "         '进口食品', '进口普通食品']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "rePatterns = []\n",
    "for i in list(reasons.keys())[:-1]:\n",
    "    rePatterns.append([{\"ORTH\": {\"IN\": reasons[i]}}])\n",
    "# rePatterns\n",
    "importPattern = [{\"ORTH\": {\"IN\": reasons['keyword1']}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"zh_core_web_lg\")\n",
    "# case reasons matcher\n",
    "reMatcher = Matcher(nlp.vocab)\n",
    "for i in range(1, len(rePatterns)+1):\n",
    "    reMatcher.add(\"reason\"+str(i), [rePatterns[i-1]])\n",
    "    nlp.tokenizer.pkuseg_update_user_dict(reasons[list(reasons.keys())[i-1]])\n",
    "\n",
    "keywordMatcher = Matcher(nlp.vocab)\n",
    "keywordMatcher.add(\"isImport\", [importPattern])\n",
    "nlp.tokenizer.pkuseg_update_user_dict(reasons['keyword1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def extract_reasons(path):\n",
    "    reasonDict = {}\n",
    "    isImport = False\n",
    "    with open(path, encoding = \"UTF-8\") as f:\n",
    "        data = json.load(f)\n",
    "        dict_data = json.loads(data)\n",
    "        case_id = dict_data['returnData']['id']\n",
    "        for i in dict_data['returnData']['segments']:\n",
    "            if (i['type'] == \"ASCERTAIN\" or i['type'] == \"COURT_HELD\" or i['type'] == \"courtHeld\"):\n",
    "                doc = nlp(i['text'])\n",
    "                matcher = reMatcher(doc)\n",
    "                kmatcher = keywordMatcher(doc)\n",
    "                \n",
    "                #如果有匹配到pattern\n",
    "                if len(matcher) > 0:\n",
    "                    for match_id, start, end in matcher:\n",
    "                        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "                        if string_id not in reasonDict.keys():\n",
    "                            reasonDict[string_id] = 0\n",
    "                        reasonDict[string_id] += 1\n",
    "                if len(kmatcher) > 0:\n",
    "                    isImport = True\n",
    "        sortReasons = sorted(reasonDict.items(), key=lambda x: x[1], reverse=True)\n",
    "        if len(sortReasons)==0: return case_id, [], False#空值可以在后续pandas dropna处理掉\n",
    "        \n",
    "        # reason dict filter\n",
    "        reasonList = list(reasonDict.keys())\n",
    "        maxTimes = sortReasons[0][1]\n",
    "        if maxTimes > 2:\n",
    "            for v in sortReasons:\n",
    "                if (v[1] <= 2 or maxTimes-v[1] >3):\n",
    "                    reasonList.remove(v[0])\n",
    "        else:\n",
    "            reasonList = [sortReasons[0][0]]\n",
    "    return case_id, reasonList, isImport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('57baba77c2265c5f452d2f3a', ['reason11'], False)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test extract_reasons\n",
    "extract_reasons('/Users/starice/OwnFiles/cityu/RA/type1/2014/1/json/57baba77c2265c5f452d2f3a.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bath_extract_reasons(dir_path):\n",
    "    reasonsPd = pd.DataFrame(columns=[\"case_id\", \"case_reason\", \"is_import\"])\n",
    "    files = os.listdir(dir_path)\n",
    "    for file in files: \n",
    "        if os.path.splitext(file)[-1][1:] != \"json\": continue\n",
    "        if not os.path.isdir(file): #判断是否是文件夹，不是文件夹才打开\n",
    "            file_path = dir_path + \"/\" + file\n",
    "            case_id, reasonList, isImport = extract_reasons(file_path)\n",
    "            reasonsPd = reasonsPd.append({\"case_id\": case_id, \"case_reason\": reasonList, \"is_import\": isImport}, \n",
    "                                        ignore_index=True)\n",
    "    return reasonsPd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_cases(pre_dir, dir_name, dir_sname):\n",
    "    for i in pre_dir:\n",
    "        for j in dir_name:\n",
    "            for k in dir_sname:\n",
    "                dir_path = base_url + i + \"/\" + j + \"/\" + k + \"/json\"\n",
    "                print(\"processing directory: \", dir_path)\n",
    "                if not os.path.exists(dir_path):\n",
    "                    print(\"路径不存在！\", dir_path)\n",
    "                    continue\n",
    "                reasonsPd = bath_extract_reasons(dir_path)\n",
    "                reasonsPd.to_csv(\"/Users/starice/Desktop/case_reasons/\" + str(i) + \"_\" + str(j) + \"_\" + str(k) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/1/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/2/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/3/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/4/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/5/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/6/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/7/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/8/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/9/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/10/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/11/json\n",
      "processing directory:  /Users/starice/OwnFiles/cityu/RA/type2/2017/12/json\n"
     ]
    }
   ],
   "source": [
    "process_multiple_cases(pre_dir[1:2], dir_name[3:4], dir_sname[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
