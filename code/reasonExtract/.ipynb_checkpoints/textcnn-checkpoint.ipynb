{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Use TextCNN to learn the feature of COURT_HELD text\n",
    "reason topics we have:\n",
    "\t标签、配料表、外包装违规（包括虚假、夸大信息，格式、名称规范问题等等）;\n",
    "\t假冒产品（假酒、假保健品之类的）;\n",
    "\t保质期、生产日期;\n",
    "\t原材料、添加剂;\n",
    "\t商标;\n",
    "\t出入境检验、检疫证明;\n",
    "\t生产许可证、生产标准、证明文件;\n",
    "\t餐饮食品安全卫生标准;（只限餐饮）\n",
    "\t是否为消费者（有些人是专业打假人）;\n",
    "\t进口食品相关产品尚无国家标准（对尚无国家标准的食品或相关产品未做安全性评估就销售）;\n",
    "\t产品质检不合格、质量有问题;\n",
    "\t不明确（可以当做干扰项去掉）\n",
    "标签优先提取\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels in this dataset are:  ['原材料、添加剂', '产品质检不合格、质量有问题', '标签、配料表、外包装违规', '进口食品相关产品尚无国家标准', '保质期、生产日期', '生产许可证、生产标准、证明文件', '假冒产品', '餐饮食品安全卫生标准', '出入境检验、检疫证明', '商标', '是否为消费者']\n"
     ]
    }
   ],
   "source": [
    "path1 = \"/Users/starice/OwnFiles/cityu/RA/case_study/nlp_tasks/dataset/textcnn_dataset.csv\"\n",
    "path2 = \"/Users/starice/Dropbox/My Mac (Starice’s MacBook Pro)/Desktop/noun_phrases/type1_2014_1-2-3-4-5-6-7-8-9-10-11-12_nps.csv\"\n",
    "\n",
    "all_classes = pd.read_csv(path1, encoding=\"utf-8\")\n",
    "splitted_text = pd.read_csv(path2, encoding=\"utf-8\")\n",
    "all_classes = all_classes[~all_classes['reason_topic'].isin([\"不明确\", \"unknown\"])][['id', 'reason_topic']]\n",
    "splitted_text = splitted_text[['id', 'content', 'topic_phrases', 'phrase_vectors']]\n",
    "all_cases = all_classes.merge(splitted_text, on=\"id\")\n",
    "all_cases['reason_topic'] = all_cases['reason_topic'].apply(lambda row: row.split(\";\")[0])\n",
    "all_cases.rename(columns={\"reason_topic\": \"label\"}, inplace=True)\n",
    "\n",
    "labels = list(all_cases['label'].unique())\n",
    "print(\"All labels in this dataset are: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'原材料、添加剂': 50,\n",
       "         '产品质检不合格、质量有问题': 21,\n",
       "         '标签、配料表、外包装违规': 139,\n",
       "         '进口食品相关产品尚无国家标准': 3,\n",
       "         '保质期、生产日期': 64,\n",
       "         '生产许可证、生产标准、证明文件': 41,\n",
       "         '假冒产品': 13,\n",
       "         '餐饮食品安全卫生标准': 52,\n",
       "         '出入境检验、检疫证明': 5,\n",
       "         '商标': 1,\n",
       "         '是否为消费者': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(all_cases.label))\n",
    "#存在有些标签数据量过少的现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_phrases</th>\n",
       "      <th>phrase_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57ab9058c2265c28a560195d</td>\n",
       "      <td>原材料、添加剂</td>\n",
       "      <td>上述认定 证据 当事人 陈述 本院 认定 事实 2012年8月16日 喻忠 淘宝网 天猫商城...</td>\n",
       "      <td>['上述认定', '证据', '当事人', '本院', '事实', '淘宝网天猫商城', '...</td>\n",
       "      <td>[b'~\\x8c\\x1f@\\x88\\xf4\\x17\\xc0\\x98\\xf5B\\xbf;S0?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57baba28c2265c5f452d2cef</td>\n",
       "      <td>产品质检不合格、质量有问题</td>\n",
       "      <td>本院认为,原告购买的是被告 散装茶叶 选用 被告 店 市场 流行 通用包装盒 装 茶 原告 ...</td>\n",
       "      <td>['散装茶叶', '市场', '通用包装盒', '收据', '当庭陈述', '质量安全管理'...</td>\n",
       "      <td>[b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57abb74ac2265c258984430a</td>\n",
       "      <td>标签、配料表、外包装违规</td>\n",
       "      <td>本院认为,食品安全标准 应 包括 食品安全 营养 标签 标识 说明书 食品经营者 查验 商品...</td>\n",
       "      <td>['食品安全', '营养', '标签', '标识', '食品经营者', '商品合格', '证...</td>\n",
       "      <td>[b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57baba99c2265c5f452d3090</td>\n",
       "      <td>产品质检不合格、质量有问题</td>\n",
       "      <td>本院认为,原告购买的是被告 散装茶叶 选用 被告 店 市场 流行 通用包装盒 装 茶 原告 ...</td>\n",
       "      <td>['散装茶叶', '市场', '通用包装盒', '收据', '当庭陈述', '质量安全管理'...</td>\n",
       "      <td>[b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57baba8fc2265c5f452d303f</td>\n",
       "      <td>产品质检不合格、质量有问题</td>\n",
       "      <td>本院认为,原告购买的是被告 散装茶叶 选用 被告 店 市场 流行 通用包装盒 装 茶 原告 ...</td>\n",
       "      <td>['散装茶叶', '市场', '通用包装盒', '收据', '当庭陈述', '质量安全管理'...</td>\n",
       "      <td>[b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id          label  \\\n",
       "0  57ab9058c2265c28a560195d        原材料、添加剂   \n",
       "1  57baba28c2265c5f452d2cef  产品质检不合格、质量有问题   \n",
       "2  57abb74ac2265c258984430a   标签、配料表、外包装违规   \n",
       "4  57baba99c2265c5f452d3090  产品质检不合格、质量有问题   \n",
       "6  57baba8fc2265c5f452d303f  产品质检不合格、质量有问题   \n",
       "\n",
       "                                             content  \\\n",
       "0  上述认定 证据 当事人 陈述 本院 认定 事实 2012年8月16日 喻忠 淘宝网 天猫商城...   \n",
       "1  本院认为,原告购买的是被告 散装茶叶 选用 被告 店 市场 流行 通用包装盒 装 茶 原告 ...   \n",
       "2  本院认为,食品安全标准 应 包括 食品安全 营养 标签 标识 说明书 食品经营者 查验 商品...   \n",
       "4  本院认为,原告购买的是被告 散装茶叶 选用 被告 店 市场 流行 通用包装盒 装 茶 原告 ...   \n",
       "6  本院认为,原告购买的是被告 散装茶叶 选用 被告 店 市场 流行 通用包装盒 装 茶 原告 ...   \n",
       "\n",
       "                                       topic_phrases  \\\n",
       "0  ['上述认定', '证据', '当事人', '本院', '事实', '淘宝网天猫商城', '...   \n",
       "1  ['散装茶叶', '市场', '通用包装盒', '收据', '当庭陈述', '质量安全管理'...   \n",
       "2  ['食品安全', '营养', '标签', '标识', '食品经营者', '商品合格', '证...   \n",
       "4  ['散装茶叶', '市场', '通用包装盒', '收据', '当庭陈述', '质量安全管理'...   \n",
       "6  ['散装茶叶', '市场', '通用包装盒', '收据', '当庭陈述', '质量安全管理'...   \n",
       "\n",
       "                                      phrase_vectors  \n",
       "0  [b'~\\x8c\\x1f@\\x88\\xf4\\x17\\xc0\\x98\\xf5B\\xbf;S0?...  \n",
       "1  [b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...  \n",
       "2  [b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0...  \n",
       "4  [b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...  \n",
       "6  [b'~\\xae\\x92?\\xe9\\xd9(@\\x9a*\\xec?Wx\\x1f\\xbe\\x1...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先去掉商标，是否为消费者，进口食品相关产品尚无国家标准这三个类别，因为数据量太小了\n",
    "all_cases = all_cases[~all_cases['label'].isin([\"商标\", \"是否为消费者\", \"进口食品相关产品尚无国家标准\"])]\n",
    "Counter(list(all_cases.label))\n",
    "all_cases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原材料、添加剂\n",
      "30 10\n",
      "产品质检不合格、质量有问题\n",
      "12 4\n",
      "标签、配料表、外包装违规\n",
      "83 27\n",
      "保质期、生产日期\n",
      "38 12\n",
      "生产许可证、生产标准、证明文件\n",
      "24 8\n",
      "假冒产品\n",
      "7 2\n",
      "餐饮食品安全卫生标准\n",
      "31 10\n",
      "出入境检验、检疫证明\n",
      "3 1\n"
     ]
    }
   ],
   "source": [
    "# 每个类别都按照311划分为train validation和test，将这三类文件输出\n",
    "textcnn_bpath = \"/Users/starice/OwnFiles/cityu/RA/case_study/nlp_tasks/dataset/\"\n",
    "train = pd.DataFrame(columns=[\"label\", \"content\"])\n",
    "val = pd.DataFrame(columns=[\"label\", \"content\"])\n",
    "test = pd.DataFrame(columns=[\"label\", \"content\"])\n",
    "for i in list(all_cases.label.unique()):\n",
    "    print(i)\n",
    "    train_len = int(len(all_cases[all_cases['label']==i]) * 0.6)\n",
    "    val_len = int(len(all_cases[all_cases['label']==i]) * 0.2)\n",
    "    # test_len = len(all_cases[all_cases['label']==i]) * 0.2\n",
    "    print(train_len, val_len)\n",
    "    train = train.append(all_cases[all_cases['label']==i][:train_len][['label', 'content']], ignore_index=True)\n",
    "    val = val.append(all_cases[all_cases['label']==i][train_len:train_len+val_len][['label', 'content']], ignore_index=True)\n",
    "    test = test.append(all_cases[all_cases['label']==i][train_len+val_len:][['label', 'content']], ignore_index=True)\n",
    "with open(textcnn_bpath+\"train.tsv\",'w') as write_tsv:\n",
    "    write_tsv.write(train.to_csv(sep='\\t', index=False))\n",
    "with open(textcnn_bpath+\"val.tsv\",'w') as write_tsv:\n",
    "    write_tsv.write(val.to_csv(sep='\\t', index=False))\n",
    "with open(textcnn_bpath+\"test.tsv\",'w') as write_tsv:\n",
    "    write_tsv.write(test.to_csv(sep='\\t', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#声明一个Field对象，对象里面填的就是需要对文本进行哪些操作，比如这里lower=True英文大写转小写,tokenize=cut对于文本分词采用之前定义好的cut函数，sequence=True表示输入的是一个sequence类型的数据，还有其他更多操作可以参考文档\n",
    "TEXT = data.Field(sequential=True,lower=True)\n",
    "#声明一个标签的LabelField对象，sequential=False表示标签不是sequence，dtype=torch.int64标签转化成整形\n",
    "LABEL = data.LabelField(sequential=False, dtype=torch.int64)\n",
    "\n",
    "#这里主要是告诉torchtext需要处理哪些数据，这些数据存放在哪里，TabularDataset是一个处理scv/tsv的常用类\n",
    "train_dataset,dev_dataset,test_dataset = data.TabularDataset.splits(\n",
    "      path=textcnn_bpath,  #文件存放路径\n",
    "      format='tsv',   #文件格式\n",
    "      skip_header=True,  #是否跳过表头，我这里数据集中没有表头，所以不跳过\n",
    "      train='train.tsv',\n",
    "      validation='val.tsv',\n",
    "      test='test.tsv',\n",
    "      fields=[('label',LABEL),('content',TEXT)] # 定义数据对应的表头\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 预训练词向量\n",
    "pretrained_name = 'sgns.baidubaike.bigram-char' # 预训练词向量文件名\n",
    "pretrained_path = textcnn_bpath #预训练词向量存放路径\n",
    "vectors = torchtext.vocab.Vectors(name=pretrained_name, cache=pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 建立词表\n",
    "TEXT.build_vocab(train_dataset,\n",
    "                 dev_dataset,\n",
    "                 test_dataset,\n",
    "                 vectors=vectors)\n",
    "LABEL.build_vocab(train_dataset,\n",
    "                  dev_dataset,\n",
    "                  test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表中的词汇总量为：  8621\n",
      "词向量维度为：  torch.Size([8621, 300])\n",
      "6558\n"
     ]
    }
   ],
   "source": [
    "# 查看词表信息\n",
    "print(\"词表中的词汇总量为： \", len(TEXT.vocab))\n",
    "print(\"词向量维度为： \", TEXT.vocab.vectors.shape)\n",
    "print(TEXT.vocab.stoi['标签不符合食品安全规定'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#生成迭代器\n",
    "train_iter, dev_iter,test_iter = data.BucketIterator.splits(\n",
    "        (train_dataset, dev_dataset,test_dataset), #需要生成迭代器的数据集\n",
    "        batch_sizes=(128, 128,128), # 每个迭代器分别以多少样本为一个batch\n",
    "        sort_key=lambda x: len(x.content) #按什么顺序来排列batch，这里是以句子的长度，就是上面说的把句子长度相近的放在同一个batch里面\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TextCNN建模\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 class_num, # 最后输出的种类数\n",
    "                 filter_sizes, # 卷积核的长也就是滑动窗口的长\n",
    "                 filter_num,   # 卷积核的数量\n",
    "                 vocabulary_size, # 词表的大小\n",
    "                 embedding_dimension, # 词向量的维度\n",
    "                 vectors, # 词向量\n",
    "                 dropout): # dropout率\n",
    "        super(TextCNN, self).__init__() # 继承nn.Module\n",
    "\n",
    "        chanel_num = 1  # 通道数，也就是一篇文章一个样本只相当于一个feature map\n",
    "\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension) # 嵌入层\n",
    "        self.embedding = self.embedding.from_pretrained(vectors) #嵌入层加载预训练词向量\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(chanel_num, filter_num, (fsz, embedding_dimension)) for fsz in filter_sizes])  # 卷积层\n",
    "        self.dropout = nn.Dropout(dropout) # dropout\n",
    "        self.fc = nn.Linear(len(filter_sizes) * filter_num, class_num) #全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x维度[句子长度,一个batch中所包含的样本数] 例:[3451,128]\n",
    "        x = self.embedding(x) # #经过嵌入层之后x的维度，[句子长度,一个batch中所包含的样本数,词向量维度] 例：[3451,128,300]\n",
    "        x = x.permute(1,0,2) # permute函数将样本数和句子长度换一下位置，[一个batch中所包含的样本数,句子长度,词向量维度] 例：[128,3451,300]\n",
    "        x = x.unsqueeze(1) # # conv2d需要输入的是一个四维数据，所以新增一维feature map数 unsqueeze(1)表示在第一维处新增一维，[一个batch中所包含的样本数,一个样本中的feature map数，句子长度,词向量维度] 例：[128,1,3451,300]\n",
    "        x = [conv(x) for conv in self.convs] # 与卷积核进行卷积，输出是[一个batch中所包含的样本数,卷积核数，句子长度-卷积核size+1,1]维数据,因为有[3,4,5]三张size类型的卷积核所以用列表表达式 例：[[128,16,3459,1],[128,16,3458,1],[128,16,3457,1]]\n",
    "        x = [sub_x.squeeze(3) for sub_x in x]#squeeze(3)判断第三维是否是1，如果是则压缩，如不是则保持原样 例：[[128,16,3459],[128,16,3458],[128,16,3457]]\n",
    "        x = [F.relu(sub_x) for sub_x in x] # ReLU激活函数激活，不改变x维度\n",
    "        x = [F.max_pool1d(sub_x,sub_x.size(2)) for sub_x in x] # 池化层，根据之前说的原理，max_pool1d要取出每一个滑动窗口生成的矩阵的最大值，因此在第二维上取最大值 例：[[128,16,1],[128,16,1],[128,16,1]]\n",
    "        x = [sub_x.squeeze(2) for sub_x in x] # 判断第二维是否为1，若是则压缩 例：[[128,16],[128,16],[128,16]]\n",
    "        x = torch.cat(x, 1) # 进行拼接，例：[128,48]\n",
    "        x = self.dropout(x) # 去除掉一些神经元防止过拟合，注意dropout之后x的维度依旧是[128,48]，并不是说我dropout的概率是0.5，去除了一半的神经元维度就变成了[128,24]，而是把x中的一些神经元的数据根据概率全部变成了0，维度依旧是[128,48]\n",
    "        logits = self.fc(x) # 全接连层 例：输入x是[128,48] 输出logits是[128,10]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "class_num = len(LABEL.vocab) # 类别数目\n",
    "filter_size = [3,4,5]  # 卷积核种类数\n",
    "filter_num=16   # 卷积核数量\n",
    "vocab_size = len(TEXT.vocab) # 词表大小\n",
    "embedding_dim = TEXT.vocab.vectors.size()[-1] # 词向量维度\n",
    "vectors = TEXT.vocab.vectors # 词向量\n",
    "dropout=0.5\n",
    "learning_rate = 0.001  # 学习率\n",
    "epochs = 5   # 迭代次数\n",
    "save_dir = '/Users/starice/OwnFiles/cityu/RA/case_study/nlp_tasks/model' # 模型保存路径\n",
    "steps_show = 10   # 每10步查看一次训练集loss和mini batch里的准确率\n",
    "steps_eval = 100  # 每100步测试一下验证集的准确率\n",
    "early_stopping = 1000  # 若发现当前验证集的准确率在1000步训练之后不再提高 一直小于best_acc,则提前停止训练\n",
    "\n",
    "textcnn_model = TextCNN(class_num=class_num,\n",
    "        filter_sizes=filter_size,\n",
    "        filter_num=filter_num,\n",
    "        vocabulary_size=vocab_size,\n",
    "        embedding_dimension=embedding_dim,\n",
    "        vectors=vectors,\n",
    "        dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dev_eval(dev_iter,model):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in dev_iter:\n",
    "        feature, target = batch.content, batch.label\n",
    "        if torch.cuda.is_available():\n",
    "            feature, target = feature.cuda(), target.cuda()\n",
    "        logits = model(feature)\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logits, 1)\n",
    "                    [1].view(target.size()).data == target.data).sum()\n",
    "    size = len(dev_iter.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss,\n",
    "                                                                          accuracy,\n",
    "                                                                          corrects,\n",
    "                                                                          size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 定义模型保存函数\n",
    "def save(model, save_dir, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = 'bestmodel_steps{}.pt'.format(steps)\n",
    "    save_bestmodel_path = os.path.join(save_dir, save_path)\n",
    "    torch.save(model.state_dict(), save_bestmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_iter, dev_iter, model):\n",
    "\n",
    "    if torch.cuda.is_available(): # 判断是否有GPU，如果有把模型放在GPU上训练，速度质的飞跃\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # 梯度下降优化器，采用Adam\n",
    "    steps = 0\n",
    "    best_acc = 0\n",
    "    last_step = 0\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for batch in train_iter:\n",
    "            feature, target = batch.content, batch.label\n",
    "            if torch.cuda.is_available(): # 如果有GPU将特征更新放在GPU上\n",
    "                feature,target = feature.cuda(),target.cuda()\n",
    "            optimizer.zero_grad() # 将梯度初始化为0，每个batch都是独立训练地，因为每训练一个batch都需要将梯度归零\n",
    "            logits = model(feature)\n",
    "            loss = F.cross_entropy(logits, target) # 计算损失函数 采用交叉熵损失函数\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step() # 放在loss.backward()后进行参数的更新\n",
    "            steps += 1\n",
    "            if steps % steps_show == 0: # 每训练多少步计算一次准确率，我这边是1，可以自己修改\n",
    "                corrects = (torch.max(logits, 1)[1].view(target.size()).data == target.data).sum() # logits是[128,10],torch.max(logits, 1)也就是选出第一维中概率最大的值，输出为[128,1],torch.max(logits, 1)[1]相当于把每一个样本的预测输出取出来，然后通过view(target.size())平铺成和target一样的size (128,),然后把与target中相同的求和，统计预测正确的数量\n",
    "                train_acc = 100.0 * corrects / batch.batch_size # 计算每个mini batch中的准确率\n",
    "                print('steps:{} - loss: {:.6f}  acc:{:.4f}'.format(\n",
    "                  steps,\n",
    "                  loss.item(),\n",
    "                  train_acc))\n",
    "\n",
    "            if steps % steps_eval == 0: # 每训练100步进行一次验证\n",
    "                dev_acc = dev_eval(dev_iter,model)\n",
    "                if dev_acc > best_acc:\n",
    "                    best_acc = dev_acc\n",
    "                    last_step = steps\n",
    "                    print('Saving best model, acc: {:.4f}%\\n'.format(best_acc))\n",
    "                    save(model,save_dir, steps)\n",
    "                else:\n",
    "                    if steps - last_step >= early_stopping:\n",
    "                        print('\\n提前停止于 {} steps, acc: {:.4f}%'.format(last_step, best_acc))\n",
    "                    raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-178a051bf35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextcnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "train(train_iter, dev_iter, textcnn_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
