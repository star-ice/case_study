{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import zh_core_web_sm\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import re\n",
    "from spacy.tokens import Doc\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy import displacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.symbols import POS\n",
    "from spacy.strings import StringStore\n",
    "from spacy.pipeline import Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.709 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "<spacy.lang.zh.ChineseTokenizer object at 0x7fa5cb11a940>\n"
     ]
    }
   ],
   "source": [
    "#小试牛刀\n",
    "nlp = zh_core_web_sm.load()\n",
    "# nlp = Chinese()\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.tokenizer)\n",
    "\n",
    "# # 输出识别出来的实体\n",
    "# for d in doc.ents:\n",
    "#     print(d.text, d.label_)\n",
    "\n",
    "# #？？？待解释\n",
    "# for tok in doc:\n",
    "#     print(tok.text, \"...\", tok.dep_)\n",
    "\n",
    "# for chunk in doc.noun_chunks:\n",
    "#     print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "#             chunk.root.head.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['住所地']\n"
     ]
    }
   ],
   "source": [
    "# 使用自定义matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"TEXT\": \"住所\"}, {\"TEXT\": \"地\"}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"residence\", None, pattern)\n",
    "\n",
    "# Use the matcher on the doc\n",
    "doc = nlp(\"住所地在山西省大同市\")\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用指定的string list作为matcher词库去匹配\n",
    "party_titles = ['三被告共同委托代理人', '两被申诉人的共同委托代理人', '被上诉人(原审原告、反诉被告)', '上述两被告的共同委托诉讼代理人', '代表人', '公司代表人', '法定代表人', '特别授权被告', '上诉人(原审第三人)', '上述两被告委托代理人', '上述两被告共同委 托诉讼代理人', '再审申请人(原审被告)', '负责人', '委托代理人(特别授权)', '被申请人(一审被告、二审被上诉人)', '委托诉讼代理人(特别授权)', '被告委托代理人', '再审申请人', '诉讼委托代理人', '被告一', '拟稿人', '被上诉人(原审被告)', '上诉人(原审原告)', '被上诉人二(原审被告二)', '两被告共同委托代理人', '上诉人共同委托代理人', '原审被告', '被告二', '上诉人(原审被告、反诉原告)', '三上诉人的共同委托代理人', ' 再审申请人(一审原告)', '申诉人(一审原告、二审上诉人)', '支持起诉人', '两上诉人共同委托的诉讼代理人', '原审第二被告', '被上诉人一(原审被告一)', '一审被告', '原告', '被上诉人共同委托代理人', '原审第三人 ', '两原审被告共同委托代理人', '被告', '上述三上诉人共同委托代理人', '被上诉人(一审原告)', '授权代理人', '诉讼代表人', '被告(反诉原告)', '法人代表', '法人代表人', '法定代表人(负责人)', '两被上诉人共同的委托代理人', '上述两被告的委托 诉讼代理人', '申诉人(一审原告、二审被上诉人)', '(一审被告、二审被上诉人)', '一审第三人', '上诉人', '以上原告共同委托代理人', '以上二被上诉人的共同委托诉讼代理人', '申请再审人(原审被告)', '公益诉讼出庭人', '以上二被告共同委托代理人', '以上三被告共同委托代理人', '上述两被告的共同委托代理人', '以上两被告委托代理人', '被申诉人(一审被告、二审上诉人)', '两被上诉人的共同委托代理人', '二被告之共同委托诉讼代理人', '共同委托诉讼代理人', '两被告的共同委托代理人', '支持起诉机关', '上列两被上诉人委托代理人 ', '上述两被上诉人的共同委托诉讼代理人', '二被告共同委托诉讼代理人', '上列两被上诉人的委托代理人', ' 两被告共同的委托代理人', '上述两被告共同委托代理人', '被上诉人的共同委托诉讼代理人', '以上三被告共同的委托诉讼代理人', '抗诉机关', '被上诉人', '原审第三被告', '上诉人(一审原告)', '二被告共同委托代理人', '以上两被告共同委委托代理人', '两被申诉人共同委托诉讼代理人', '特别授权委托诉讼代理人', '两上诉 人共同委托代理人', '以上两被上诉人共同委托诉讼代理人', '上诉人(原审第一被告)', '申诉人(一审原告、二审上诉人、原再审申请人)', '第二被告', '被申请人(原审被告)', '上述两上诉人共同委托代理人', '被申诉人 (一审被告、二审被上诉人、原再审被申请人)', '再审申请人(一审原告、二审上诉人)', '两上诉人的共同委托代理人', '上列两被告的共同委托代理人', '上列两被告共同委托代理人', '被申请人(一审被告)', '原告共同委托诉讼代理人', '被申请人(一审被告、二审上诉人)', '上诉人共同委托诉讼代理人', '被申诉人(一审被告、二审 被上诉人)', '被上诉人(一审被告)', '上述二上诉人的共同委托诉讼代理人', '被申诉人(原审被告)', '二被上 诉人共同委托诉讼代理人', '上述两上诉人共同的委托诉讼代理人', '第三被告', '再审申请人(一审原告、二审被 上诉人)', '两上诉人共同委托诉讼代理人', '被申请人(一审原告、二审被上诉人)', '申诉人(原审原告)', '两被上诉人共同委托诉讼代理人', '诉讼代理人', '上诉人(一审被告)', '被申请人(原审原告)', '上诉人(原审被告)', '以上二上诉人共同的委托诉讼代理人', '上述两上诉人的共同委托代理人', '两上诉人的共同委托诉讼代理人', '上述两被上诉人委托诉讼代理人', '上上诉人(原审被告)', '第三人', '法定代理人', '被上诉人(原审第三人)', '原告(反诉被告)', '第一被告', '三被上诉人共同委托诉讼代理人', '委托代理人', '被申诉人(一审被告、二审被上诉人、再审被申请人)', '委托代理人(特别授权代理)', '两第三人的共同委托代理人', '被上诉人共同委托代理人(特别授权代理)', '再审申请人(一审被告、二审上诉人)', '以上两被告共同委托 代理人', '以上两被告的委托代理人', '上述两上诉人共同委托诉讼代理人', '上述被上诉人共同委托诉讼代理人 ', '二被委托代理人', '被申请人', '两被告共同委托诉讼代理人', '公益诉讼起诉人', '被上诉人(原审原告)', '上述两被上诉人共同委托代理人', '委托诉讼代理人', '上述被告共同委托诉讼代理人', '以上二被上诉 人委托代理人', '上列两被上诉人的共同委托代理人', '共同委托代理人', '再审申请人(原审原告)', '上述上诉人的委托诉讼代理人', '申诉人(一审原告、二审上诉人、再审申请人)', '二被上诉人的委托代理人', '上述两被上诉人的共同委托代理人', '上列二被告共同委托代理人', '以上两被告共同委托诉讼代理人', '执行事务合伙人', '投资人']\n",
    "\n",
    "party_infos = ['身份证地址', '联系地址', '通信地址', '现住', '住', '信用代码', '籍贯', '住址', '住址地', '所在地', '户籍所在地', '住所', '营业地', '身份证住址', '港澳证件号码', '原名称', '注册号', '组织机构代码', '非公司私营企业住所', '户籍地址', '身份证号码', '住', '组织机构代码证', '公⺠身份号码', '营业执照', '身份号码', '营业执照注册号', '住所地', '执照注册号', '经营场所', '身份证住址', ' 个体工商户营业执照注册', '机构代码', '户籍住所', '统一信用代码', '原名称', '身份证地址', '经营地址', '身份证登记住址', '组织机构代码证号', '执业证号', '曾用名', '统一社会信用代码', '工商注册号', '身份证号', '注册号', '营业场所', '居⺠身份证', '地址', '现住址', '身份证住址', '经常居住地', '代理权限', '户籍地', '营业执照号码', '公⺠身份证号', '中文名称 ', '住址', '所在地', '户籍地址', '组织机构代码证', '公⺠身份号码', '住所地', '代码', '经营业主', '经营场所', '身份证登记住址', '统一社会信用代码', '地址', '注册号', '营业场所', '居⺠身份证号码', '户籍地', '身份证住址']\n",
    "\n",
    "party_occupations = ['律师', '经理', '总经理', '董事长', '法务', '无职业', '业主', '员工', '店长', '主管', '经营者', '董事', '职员']\n",
    "\n",
    "cost_type = ['货款','价款','受理费','赔偿金','公证费','诉讼费','购物款','上诉费','公告费', '运费',\\\n",
    "             '交通费','误工费','打印费','鉴定费','邮寄费','赔偿款','医疗费','购酒款','购药款',\\\n",
    "             '住宿费','产品质量监督检验费','其他费用', '餐饮费', '购货款', '减半', '合计', '共计']\n",
    "\n",
    "# party_gender = ['男', '女']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns_ptitles = list(nlp.pipe(party_titles))\n",
    "patterns_pinfos = list(nlp.pipe(party_infos))\n",
    "patterns_poccups = list(nlp.pipe(party_occupations))\n",
    "patterns_pcost_types = list(nlp.pipe(cost_type))\n",
    "# patterns_genders = list(nlp.pipe(party_gender))\n",
    "# matcher.add(\"PARTY_TITLES\", None, *patterns_ptitles)\n",
    "# matcher.add(\"PARTY_INFOS\", None, *patterns_pinfos)\n",
    "# matcher.add(\"PARTY_OCCUPS\", None, *patterns_poccups)\n",
    "# matcher.add(\"PARTY_genders\", None, *patterns_genders)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "# matches = matcher(doc)\n",
    "# print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "import sys\n",
    "TEXT = []\n",
    "with open(\"/Users/starice/Desktop/partyinfo_type2_2016_1_text.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        TEXT.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from /Users/starice/Desktop/party_title_ner\n"
     ]
    }
   ],
   "source": [
    "output_dir=\"/Users/starice/Desktop/party_title_ner\"\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "patterns_ptitles = list(nlp2.pipe(party_titles))\n",
    "patterns_pinfos = list(nlp2.pipe(party_infos))\n",
    "patterns_poccups = list(nlp2.pipe(party_occupations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#待提取的个人信息有：\n",
    "# title， 姓名， 出生日期， 性别， 职业， 各类补充信息（住址、代码等）\n",
    "# 以上除了姓名和title是必须的之外，其他信息都是可有可无，但如果有，一定要详细准确\n",
    "#如果以逗号分隔的每一小段有多个字典匹配结果，就取最长长度的匹配结果\n",
    "# TEXT = [\"原告: 胡鎏亮,男,1991年12月21日出生,汉族,住重庆市巴南区。\", \n",
    "# \"被告: 重庆好又多百货商业有限公司,住所地重庆市南岸区南坪街道南坪正街1号,组织机构代码73984719-7。\",\n",
    "# \"法定代表人: GregoryStephenForan,总经理。\",\n",
    "# \"委托代理人: 王志剑,男,1978年3月1日出生,汉族,住山东省诸城市。\",\n",
    "# \"原告: 孙文革,男,1971年6月8日,汉族,无职业,住吉林省延吉市公园街园铁委四组。\",\n",
    "# \"委托代理人冉某(特别授权): ,重庆睿诚律师事务所律师。\", \n",
    "# \"上诉人(原审被告): 深圳市兴兴酒业贸易有限公司,住所地广东省深圳市罗湖区。\", \n",
    "# \"申诉人（一审原告、二审上诉人）:肖飞,男,1986年2月27日出生,汉族,住湖南省洞口县。\", \n",
    "# \"被申诉人（一审被告、二审上诉人）:深圳市威利文化发展有限公司,住所地在广东省深圳市罗湖区宝岗路裕华工贸大厦1栋四层481。\", \n",
    "# \"法定代表人lll。\",\n",
    "# \"法定代表人lll。\" ,\n",
    "# \"原告刘想中,男,1976年6月21日出生。\", \n",
    "# \"法定代表人陆兆禧,董事。\", \n",
    "# \"被告浙江天猫网络有限公司,住所地浙江省杭州市余杭区。\"]\n",
    "party_dict = {}\n",
    "party_dict_list = []\n",
    "party_title_label = \"PARTY_TITLE\"\n",
    "party_title_train_data = [] #(\"原告: lllll\", {\"entities\": [(0, 3, LABEL)]})\n",
    "\n",
    "for i in TEXT:\n",
    "    i = i[:-1] #去掉句号\n",
    "    ii = i.split(\",\")\n",
    "    for iii in ii:\n",
    "        # print(iii)\n",
    "        doc = nlp2(iii)\n",
    "\n",
    "        #出生日期\n",
    "        re_date = r\"\\d{2,4}年\\d{1,2}月\\d{1,2}日\"\n",
    "        date_m = re.findall(re_date, iii)\n",
    "        if len(date_m) > 0:\n",
    "            party_dict['birth_date'] = date_m[0]\n",
    "        #性别\n",
    "        re_gender = r\"(男|女)\"\n",
    "        gender_m = re.findall(re_gender, iii)\n",
    "        if len(gender_m)>0:\n",
    "            party_dict['gender'] = gender_m[0]\n",
    "\n",
    "        #title+姓名\n",
    "        # print(\"processing party title\\n\")\n",
    "        # party_dict['title'] = \"未知\"\n",
    "        # party_dict['name'] = \"未知\"\n",
    "        matcher_title = PhraseMatcher(nlp2.vocab)\n",
    "        # patterns_ptitles = list(nlp2.pipe(party_titles))\n",
    "        matcher_title.add(\"PARTY_TITLES\", None, *patterns_ptitles)\n",
    "        matches_title = matcher_title(doc)\n",
    "        # print(matches_title)\n",
    "        if len(matches_title) > 0:\n",
    "            temp_matches = [end-start for match_id, start, end in matches_title]\n",
    "            # print(matches_title)\n",
    "            #如果有多个匹配结果，就取长度最长的匹配作为最终结果\n",
    "            final_index = temp_matches.index(max(temp_matches))\n",
    "            temp_party_result = [(doc[start:end], (start, end)) for match_id, start, end in matches_title][final_index]\n",
    "            temp_key = str(temp_party_result[0])\n",
    "            party_name = str(doc[temp_party_result[1][1]:]).replace(\":\", \"\")\n",
    "            # party_dict[temp_key] = party_name\n",
    "            party_dict['title'] = temp_key\n",
    "            party_dict['name'] = party_name\n",
    "            party_title_train_data.append((iii, {\"entities\": [(iii.index(temp_party_result[0].text), \\\n",
    "                iii.index(temp_party_result[0].text) + len(temp_party_result[0].text), party_title_label)]}))\n",
    "\n",
    "        #职业\n",
    "        # party_dict['occupation'] = \"未知\"\n",
    "        # print(\"processing occupation\\n\")\n",
    "        matcher_occup = PhraseMatcher(nlp.vocab)\n",
    "        # patterns_poccups = list(nlp.pipe(party_occupations))\n",
    "        matcher_occup.add(\"PARTY_OCCUPS\", None, *patterns_poccups)\n",
    "        matches_occup = matcher_occup(doc)\n",
    "        if len(matches_occup) > 0:\n",
    "            temp_matches = [end-start for match_id, start, end in matches_occup]\n",
    "            final_index = temp_matches.index(max(temp_matches))\n",
    "            temp_party_result =  [(doc[start:end], (start, end)) for match_id, start, end in matches_occup][final_index]\n",
    "            party_dict['occupation'] = str(temp_party_result[0])\n",
    "        \n",
    "        #其他补充信息\n",
    "        # print(\"processing party info\\n\")\n",
    "        matcher_info = PhraseMatcher(nlp.vocab)\n",
    "        # patterns_pinfos = list(nlp.pipe(party_infos))\n",
    "        matcher_info.add(\"PARTY_INFOS\", None, *patterns_pinfos)\n",
    "        matches_info = matcher_info(doc)\n",
    "        if len(matches_info) > 0:\n",
    "            temp_matches = [end-start for match_id, start, end in matches_info]\n",
    "            final_index = temp_matches.index(max(temp_matches))\n",
    "            temp_party_result =  [(doc[start:end], (start, end)) for match_id, start, end in matches_info][final_index]\n",
    "            temp_key = str(temp_party_result[0])\n",
    "            temp_info = str(doc[temp_party_result[1][1]:])\n",
    "            party_dict[temp_key] = temp_info\n",
    "    \n",
    "    # print(party_dict)\n",
    "    # print()\n",
    "    # print(party_title_train_data)\n",
    "    # print()\n",
    "    party_dict_list.append(party_dict)\n",
    "    party_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the training data for ner model\n",
    "with open(\"/Users/starice/Desktop/partytitle_train_第二类_2016_11_text.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    for i in party_title_train_data:\n",
    "        f.write(str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the party title ner model we trained in train_ner.py and saved to party_title_ner dir\n",
    "test_text = [\n",
    "\"原告: 胡鎏亮,男,1991年12月21日出生,汉族,住重庆市巴南区。\", \n",
    "\"被告: 重庆好又多百货商业有限公司,住所地重庆市南岸区南坪街道南坪正街1号,组织机构代码73984719-7。\",\n",
    "\"法定代表人: GregoryStephenForan,总经理。\",\n",
    "\"委托代理人: 王志剑,男,1978年3月1日出生,汉族,住山东省诸城市。\",\n",
    "\"原告: 孙文革,男,1971年6月8日,汉族,无职业,住吉林省延吉市公园街园铁委四组。\",\n",
    "\"委托代理人冉某(特别授权): ,重庆睿诚律师事务所律师。\", \n",
    "\"上诉人(原审被告): 深圳市兴兴酒业贸易有限公司,住所地广东省深圳市罗湖区。\", \n",
    "\"申诉人（一审原告、二审上诉人）:肖飞,男,1986年2月27日出生,汉族,住湖南省洞口县。\", \n",
    "\"被申诉人（一审被告、二审上诉人）:深圳市威利文化发展有限公司,住所地在广东省深圳市罗湖区宝岗路裕华工贸大厦1栋四层481。\", \n",
    "\"法定代表人lll。\",\n",
    "\"法定代表人lll。\" ,\n",
    "\"原告刘想中,男,1976年6月21日出生。\", \n",
    "\"法定代表人陆兆禧,董事。\", \n",
    "\"被告浙江天猫网络有限公司,住所地浙江省杭州市余杭区。\"\n",
    "]\n",
    "\n",
    "output_dir=\"/Users/starice/Desktop/party_title_ner\"\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "patterns_ptitles = list(nlp2.pipe(party_titles))\n",
    "patterns_pinfos = list(nlp2.pipe(party_infos))\n",
    "patterns_poccups = list(nlp2.pipe(party_occupations))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_text:\n",
    "    ii = i.split(\",\")\n",
    "    for iii in ii:\n",
    "        doc2 = nlp2(iii)\n",
    "        print(doc2.text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "        matcher_title = PhraseMatcher(nlp2.vocab)\n",
    "        matcher_title.add(\"PARTY_TITLES\", None, *patterns_ptitles)\n",
    "        matches_title = matcher_title(doc2)\n",
    "        if len(matches_title) > 0:\n",
    "            temp_matches = [end-start for match_id, start, end in matches_title]\n",
    "            # print(matches_title)\n",
    "            #如果有多个匹配结果，就取长度最长的匹配作为最终结果\n",
    "            final_index = temp_matches.index(max(temp_matches))\n",
    "            temp_party_result = [(doc2[start:end], (start, end)) for match_id, start, end in matches_title][final_index]\n",
    "            temp_key = str(temp_party_result[0])\n",
    "            party_name = str(doc2[temp_party_result[1][1]:]).replace(\":\", \"\")\n",
    "#             print(temp_key, party_name)\n",
    "            # party_dict['title'] = temp_key\n",
    "            # party_dict['name'] = party_name\n",
    "            # party_title_train_data.append((iii, {\"entities\": [(iii.index(temp_party_result[0].text), \\\n",
    "            #     iii.index(temp_party_result[0].text) + len(temp_party_result[0].text), party_title_label)]}))\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dependency parsing on judgement result text</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/Users/starice/Desktop/token_dict_第二类_2016_8.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "#     tokens_dict = eval(f.read())\n",
    "# print(tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将原告被告信息添加至词库做实体识别\n",
    "PARTY_TITLE_TEXT = []\n",
    "party = []\n",
    "with open(\"/Users/starice/Desktop/partyinfo_result_第二类_2016_8_text.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        PARTY_TITLE_TEXT.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in PARTY_TITLE_TEXT:\n",
    "    if \"{\" in i:\n",
    "        temp_party_dict = eval(i)\n",
    "        if \"name\" in temp_party_dict:\n",
    "            party.append(temp_party_dict['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append words to user dict(like jieba.add_word) 帮助分词， 抛弃原先用jieba分词结果代替spacy分词的方法\n",
    "nlp.tokenizer.pkuseg_update_user_dict(party)\n",
    "nlp.tokenizer.pkuseg_update_user_dict(cost_type)\n",
    "nlp.tokenizer.pkuseg_update_user_dict(party_titles)\n",
    "nlp.tokenizer.pkuseg_update_user_dict(party_infos)\n",
    "#细微分词调节\n",
    "judgeresult_verbs = [\"赔偿\", \"退还\", \"负担\", \"返还\", \"支付\", \"赔付\", \"承担\", \"收取\", \"迳付\", \"给付\", \"交纳\", \"缴纳\"]\n",
    "nlp.tokenizer.pkuseg_update_user_dict(judgeresult_verbs)\n",
    "nlp.tokenizer.pkuseg_update_user_dict([\"本判决\", \"发生法律效力之日起\", \"发生法律效力后\", \\\n",
    "                                       \"本判决生效之日起\", \"本判决生效后\", \"于\", \"内\", \\\n",
    "                                       \"向\", \"计\", \"张**\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用entityruler帮助nlp识别特定的实体(费用类型、当事人等等)\n",
    "\n",
    "# 原先使用的是entity matcher 但是会有overlapping错误\n",
    "# class EntityMatcher(object):\n",
    "#     name = \"entity_matcher\"\n",
    "\n",
    "#     def __init__(self, nlp, terms, label, matcher_name):\n",
    "#         patterns = [nlp.make_doc(text) for text in terms]\n",
    "#         self.matcher = PhraseMatcher(nlp.vocab)\n",
    "#         self.matcher.add(label, None, *patterns)\n",
    "#         self.name = matcher_name\n",
    "\n",
    "#     def __call__(self, doc):\n",
    "#         matches = self.matcher(doc)\n",
    "#         for match_id, start, end in matches:\n",
    "#             span = Span(doc, start, end, label=match_id)\n",
    "#             doc.ents = list(doc.ents) + [span]\n",
    "#         return doc\n",
    "# entity_matcher = EntityMatcher(nlp, cost_type, \"COST_TYPE\", \"entity_matcher_costtype\")\n",
    "# nlp.add_pipe(entity_matcher, after=\"ner\")\n",
    "# entity_matcher_party = EntityMatcher(nlp, party, \"PARTY\", \"entity_matcher_party\")\n",
    "# nlp.add_pipe(entity_matcher_party, after=\"ner\")\n",
    "\n",
    "entityruler = EntityRuler(nlp, overwrite_ents=True)\n",
    "patterns_cost_type = [{\"label\": \"COST_TYPE\", \"pattern\": str(i)} for i in cost_type]\n",
    "patterns_party = [{\"label\": \"PARTY\", \"pattern\": str(i)} for i in party]\n",
    "patterns_party_title = [{\"label\": \"PARTY_TITLE\", \"pattern\": str(i)} for i in party_titles]\n",
    "patterns_party_info = [{\"label\": \"PARTY_INFO\", \"pattern\": str(i)} for i in party_infos]\n",
    "patterns_judgere_verbs = [{\"label\": \"JUDGE_VERB\", \"pattern\": str(i)} for i in judgeresult_verbs]\n",
    "entityruler.add_patterns(patterns_cost_type + patterns_party \\\n",
    "                         + patterns_party_title + patterns_party_info \\\n",
    "                        + patterns_judgere_verbs)\n",
    "nlp.add_pipe(entityruler, last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用regex来帮助识别金额实体\n",
    "money_re = r\"(?P<money_amount>([零一二三四五六七八九十百千万亿角元\\d+\\.,]*)(?=[角|元])[角|元])\"\n",
    "\n",
    "animal_hash = StringStore([u'MONEY_AMOUNT']) # <-- match id\n",
    "nlp.vocab.strings.add('MONEY_AMOUNT')\n",
    "\n",
    "class MoneyRecognizer(object):\n",
    "    \"\"\"Example of a spaCy v2.0 pipeline component that sets entity annotations\n",
    "    based on list of single or multiple-word company names. Companies are\n",
    "    labelled as ORG and their spans are merged into one token. Additionally,\n",
    "    ._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token\n",
    "    respectively.\"\"\"\n",
    "\n",
    "    name = \"money_amount\"  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp, label=\"MONEY_AMOUNT\"):\n",
    "        \"\"\"Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the shared vocab, get the label ID and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \"\"\"\n",
    "        \n",
    "        expression = money_re\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for match in re.finditer(expression, doc.text):\n",
    "#             print(match)\n",
    "            start, end = match.span()\n",
    "#             print(start, end)\n",
    "#             print(doc)\n",
    "            matched_span = doc.char_span(start, end, label=self.label)\n",
    "#             print(\"matched_span\", matched_span)\n",
    "            # This is a Span object or None if match doesn't map to valid token sequence\n",
    "            if matched_span is not None:\n",
    "                spans.append(matched_span)\n",
    "                doc.ents = list(doc.ents) + [matched_span]\n",
    "                for s in spans:\n",
    "                    s.merge\n",
    "        return doc\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_component = MoneyRecognizer(nlp)  # initialise component\n",
    "nlp.add_pipe(money_component, first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用自定义的pos mapping table来对一些词汇指定词性\n",
    "tagger = Tagger(nlp.vocab)\n",
    "tagger.add_label(\"退还\", {POS: 'VERB'})\n",
    "tagger.add_label(\"赔偿\", {POS: 'VERB'})\n",
    "tagger.add_label(\"负担\", {POS: 'VERB'})\n",
    "tagger.add_label(\"返还\", {POS: 'VERB'})\n",
    "tagger.add_label(\"支付\", {POS: 'VERB'})\n",
    "tagger.add_label(\"赔付\", {POS: 'VERB'})\n",
    "tagger.add_label(\"承担\", {POS: 'VERB'})\n",
    "tagger.add_label(\"收取\", {POS: 'VERB'})\n",
    "tagger.add_label(\"迳付\", {POS: 'VERB'})\n",
    "tagger.add_label(\"给付\", {POS: 'VERB'})\n",
    "tagger.add_label(\"向\", {POS: 'VERB'})\n",
    "tagger.add_label(\"受理费\", {POS: 'NOUN'})\n",
    "tagger.add_label(\"深圳乐荣超市有限公司\", {POS: 'NOUN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将 entity识别结果应用到分词\n",
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(merge_ents) #merge一般都在ner之后~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.disable_pipes(\"ner\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_t = '案件受理费50元,减半收取计25元,由被告天津市人人乐商业有限公司、被告天津市人人乐商业有限公司河西购物广场承担。'\n",
    "# temp_tt = temp_t.split(\",\")\n",
    "# for tt in temp_tt:\n",
    "#     doc_dep = nlp(tt)\n",
    "#     displacy.serve(doc_dep, style=\"dep\")\n",
    "doc_dep = nlp(\"悦家公司阳光店于本判决发生法律效力之日起10日内向朱玉娄支付赔偿金2500元;悦家公司阳光店就上述付款不能履行部分由悦家公司承担连带清偿责任。\")\n",
    "\n",
    "\n",
    "displacy.render(doc_dep, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tt in temp_tt:\n",
    "#     doc_dep = nlp(tt)\n",
    "#     for token in doc_dep:\n",
    "#         print(token.text, token.pos_, token.dep_, token.head.text, token.head.pos_,\n",
    "#                 [child for child in token.children])\n",
    "for token in doc_dep:\n",
    "        print(token.text, token.pos_, token.dep_, token.ent_type_, token.head.text, token.head.pos_,\n",
    "                [child for child in token.children])\n",
    "# print(set(doc_dep))\n",
    "        \n",
    "for chunk in doc_dep.noun_chunks:\n",
    "    print(chunk.text)\n",
    "#           , chunk.root.text, chunk.root.dep_,\n",
    "#             chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/starice/Desktop/judgeresult_第二类_2016_8_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-68e36a06bf66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#系统的提取金额类型，数目，承担人或受益人\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mJUDGERE_TEXT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/starice/Desktop/judgeresult_第二类_2016_8_text.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"UTF-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mJUDGERE_TEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/starice/Desktop/judgeresult_第二类_2016_8_text.txt'"
     ]
    }
   ],
   "source": [
    "#系统的提取金额类型，数目，承担人或受益人\n",
    "JUDGERE_TEXT = []\n",
    "with open(\"/Users/starice/Desktop/judgeresult_第二类_2016_8_text.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        JUDGERE_TEXT.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInter(a,b):\n",
    "    result = list(set(a)&set(b))\n",
    "    if result:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def extract_loader(doc):\n",
    "    verb_1 = [\"承担\", \"负担\", \"交纳\", \"缴纳\"]\n",
    "    verb_2 = [\"赔偿\", \"退还\", \"返还\", \"支付\", \"赔付\", \"收取\", \"迳付\", \"给付\"]\n",
    "    loader = []\n",
    "    if isInter(verb_1, [str(i) for i in doc]):\n",
    "        print(\"processing verb_1 type...\")\n",
    "        for token in doc:\n",
    "#             print(token.text, token.dep_, token.ent_type_, [child for child in token.children])\n",
    "            if token.dep_ == \"nsubj\" and token.ent_type_ == \"PARTY\":\n",
    "                return [token.text]\n",
    "            if (token.dep_ == \"nmod:prep\" and token.ent_type_ == \"PARTY\") or \\\n",
    "            (token.dep_ == \"conj\" and token.ent_type_ == \"PARTY\") or \\\n",
    "            (token.dep_ == \"compound:nn\" and token.ent_type_ == \"PARTY\"):\n",
    "                loader.append(token.text)\n",
    "            \n",
    "        if len(loader) == 0: #关键字匹配\n",
    "#             print(\"processing verb_1 type keyword match...\")\n",
    "            party_changed = False\n",
    "            for token in doc:\n",
    "                if token.ent_type_ == \"PARTY_TITLE\":\n",
    "                    temp_loader = token.text\n",
    "                    party_changed = True\n",
    "                if token.ent_type_ == \"JUDGE_VERB\" and token.text in verb_1:\n",
    "#                     print(\"temp_loader:\", temp_loader)\n",
    "#                     print(\"party_changed:\", party_changed)\n",
    "                    if party_changed:\n",
    "                        loader.append(temp_loader)\n",
    "                        party_changed = False\n",
    "            loader = list(set(loader))\n",
    "    else:\n",
    "        print(\"processing verb_2 type...\")\n",
    "        temp_nsubj = []\n",
    "        #如果句子开头是个介词（例如限），则找ent type 是 party 或 party_title 的 dobj\n",
    "        if doc[0].dep_ == \"dep\":\n",
    "            for token in doc:\n",
    "                if token.dep_ == \"dobj\":\n",
    "                    if token.ent_type_ == \"PARTY\" or token.ent_type_ == \"PARTY_TITLE\" \\\n",
    "                    or token.ent_type_ in [\"PARTY\", \"PARTY_TITLE\", \"ORG\", \"PROPN\", \"PERSON\"]:\n",
    "                        temp_nsubj.append(token.text)\n",
    "        else:\n",
    "            for token in doc:\n",
    "                if token.dep_ == \"nsubj\":\n",
    "                    if token.ent_type_ == \"PARTY\" or token.ent_type_ == \"PARTY_TITLE\" \\\n",
    "                    or token.ent_type_ in [\"PARTY\", \"PARTY_TITLE\", \"ORG\", \"PROPN\", \"PERSON\"]:\n",
    "                        temp_nsubj.append(token.text)\n",
    "        print(temp_nsubj)\n",
    "        if len(temp_nsubj) == 0: \n",
    "#             print(\"processing verb_2 type keyword match...\")\n",
    "            for token in doc:\n",
    "                pass\n",
    "        elif len(temp_nsubj) == 1:\n",
    "            loader.append(temp_nsubj[0])\n",
    "            return loader\n",
    "        else:\n",
    "            print(\"multiple nsubj, split the sentence by ','\")\n",
    "            sents = doc.text.split(\",\")\n",
    "            for i in sents:\n",
    "                if re.search(money_re, i) is not None:\n",
    "                    print(i)\n",
    "                    temp_doc = nlp(i)\n",
    "                    for temp_token in temp_doc:\n",
    "                        print(temp_token.text, temp_token.dep_, temp_token.ent_type_)\n",
    "                        if temp_token.dep_ == \"nsubj\" and temp_token.ent_type_ in [\"PARTY\", \\\n",
    "                                                                                   \"PARTY_TITLE\", \"ORG\", \"PROPN\", \"PERSON\"]:\n",
    "                            loader.append(temp_token.text)\n",
    "                            break\n",
    "            if len(loader) == 0:\n",
    "                loader = temp_nsubj[0]\n",
    "                    \n",
    "                \n",
    "    return loader\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 judgement result的方法\n",
    "def extract_juegement_result(t):\n",
    "    t = t.replace(\"\\n\", \"\")\n",
    "    t = t[:-1] #去掉最后一个符号\n",
    "    t = re.sub(r'[一二三四五六七八九十]、', '', t)\n",
    "    if re.search(money_re, t) is None: return\n",
    "    else:\n",
    "#         print(t)\n",
    "        if \"。\" in t: sents = t.split(\"。\")\n",
    "        elif \";\" in t: sents = t.split(\";\")\n",
    "        else: sents = [t]\n",
    "        final_result = []\n",
    "        for i in sents:\n",
    "            print(i)\n",
    "            #这里先好好处理单个句子，不考虑是针对某个案件的，先把不同种类的句子都处理好\n",
    "            #先提取(cost_type, money_amount)\n",
    "            temp_extracted_infos = [] #example: [('受理费', '25元', '重庆永辉超市有限公司')]\n",
    "            judgere_doc = nlp(i)\n",
    "            if re.search(money_re, judgere_doc.text) is not None:\n",
    "                # 输出识别出来的实体\n",
    "#                 print(judgere_doc.text)\n",
    "                has_cost_type = False\n",
    "                for d in judgere_doc.ents:\n",
    "                    print(d.text, d.label_)\n",
    "                    if d.label_ == \"COST_TYPE\":\n",
    "                        has_cost_type = True\n",
    "                        #temp_cost_type.append(d.text)\n",
    "                        temp_extracted_info = [d.text, '', '']\n",
    "                    if d.label_ == \"MONEY_AMOUNT\":\n",
    "                        if has_cost_type:\n",
    "                            temp_extracted_info[1] = d.text\n",
    "                            has_cost_type = False\n",
    "                        else:\n",
    "                            temp_extracted_info = [\"统一赔偿费用\", d.text, '']\n",
    "                        temp_extracted_infos.append(temp_extracted_info)\n",
    "                print(\"temp_extracted_infos\", temp_extracted_infos)\n",
    "                \n",
    "                loader = extract_loader(judgere_doc)\n",
    "                for tr in temp_extracted_infos:\n",
    "                    tr[2] = loader\n",
    "                final_result += temp_extracted_infos\n",
    "    return final_result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in JUDGERE_TEXT[0:1]:\n",
    "    i = \"二、被告深圳市正德一品贸易商行应于本判决生效之日起五日内支付原告王福群购物价款十倍的赔偿金人民币35800元。\"\n",
    "    print(i)\n",
    "    print(extract_juegement_result(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一个简单的信息提取示例小程序\n",
    "nlp = en_core_web_sm.load()\n",
    "text = \"The Patient report is Positive for ABC disease\"\n",
    "doc = nlp(text)\n",
    "tokens = {token.text:token for token in doc}\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "def is_attribute(token):\n",
    "    # todo: use a classifier to determine whether the token is an attrubute\n",
    "    return token.pos_ == 'ADJ'\n",
    "\n",
    "def bfs(token, predicate, max_distance=3):\n",
    "    queue = [(token, 0)]\n",
    "    while queue:\n",
    "        print(queue)\n",
    "        t, dist = queue.pop(0)\n",
    "        print(t, dist)\n",
    "        if max_distance and dist > max_distance:\n",
    "            return\n",
    "        if predicate(t):\n",
    "            return t\n",
    "        # todo: maybe, consider only specific types of dependencies or tokens\n",
    "        neighbors =  [t.head] + list(t.children)\n",
    "        print(\"neighbors: \", neighbors, \"\\n\")\n",
    "        for n in neighbors:\n",
    "            if n and n.text:\n",
    "                queue.append((n, dist+1))\n",
    "\n",
    "print(bfs(tokens['ABC'], is_attribute))  # Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
